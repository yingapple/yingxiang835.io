---
title: "AGENT AI: SURVEYING THE HORIZONS OF MULTIMODAL INTERACTION（前五章中文翻译）"
layout: post
date: 2023-01-14 00:00
# image: /assets/images/markdown.jpg
headerImage: false
tag:
- Translation
- Agent AI
- MAA
category: blog
author: Xiang Ying
description: 多模态交互的Agent AI的论文翻译
---

# AGENT AI: SURVEYING THE HORIZONS OF MULTIMODAL INTERACTION （中文翻译）

# **ABSTRACT**

多模态AI系统可能会成为我们日常生活中无处不在的存在。使这些系统更具互动性的一个有前景的方法是将它们作为代理体嵌入物理和虚拟环境中。目前，系统利用现有的基础模型作为创建实体化Agent的基本构建块。将Agent嵌入这些环境有助于模型处理和解释视觉和语境数据的能力，这对于创造更复杂、更有语境意识的AI系统至关重要。例如，一个能够感知用户动作、人类行为、环境对象、音频表达和场景的集体情绪的系统可以用来通知并指导Agent在给定环境中的回应。为了加速基于Agent的多模态智能研究，我们将“Agent AI”定义为一个能够感知视觉刺激、语言输入和其他环境基础数据的交互系统类别，并能够产生具有无限Agent的有意义的实体化行为。特别是，我们探索旨在通过整合外部知识、多感官输入和人类反馈来改进基于下一步实体化行动预测的Agent的系统。我们认为，通过在具体环境中开发具有代理性的AI系统，也可以减轻大型基础模型的幻觉及其产生环境不正确输出的倾向。正在兴起的Agent AI领域涵盖了多模态互动的更广泛的实体化和代理性方面。除了代理在物理世界中的行动和互动外，我们设想了一个未来，在这个未来，人们可以轻松创建任何虚拟现实或模拟场景，并与嵌入虚拟环境中的Agent互动。

# 1 INTRODUCTION

## **1.1 Motivation**

在1956年达特茅斯会议上，AI系统最初被定义为能够从环境中收集信息并以有用的方式与之互动的人工生命形式。受此定义的启发，Minsky在麻省理工学院的小组在1970年建造了一个名为“拷贝演示”的机器人系统，该系统观察了“块世界”的场景，并成功地重建了观察到的多面体块结构。该系统包括观察、规划和操作模块，揭示了这些子问题各自是非常具有挑战性的，并且还需要进一步的研究。AI领域分化为专门的子领域，这些子领域在独立应对这些问题以及其他问题上取得了很大进展，但过度还原主义模糊了AI研究的总体目标。

为了超越现状，有必要回归基于亚里士多德整体论的AI基础。幸运的是，最近在大型语言模型（LLMs）和视觉语言模型（VLMs）方面的革命使得创造与整体理想一致的新型AI代理成为可能。抓住这个机会，本文探讨了集成语言能力、视觉认知、语境记忆、直觉推理和适应性的模型。它探讨了使用LLMs和VLMs完成这种整体合成的潜力。在我们的探讨中，我们还根据亚里士多德的最终原因重新审视了系统设计，即系统存在的目的，这在之前的AI发展中可能被忽视了。

强大的预训练LLMs和VLMs的出现催生了自然语言处理和计算机视觉领域的复兴。LLMs如今展现了令人印象深刻的能力来解读现实世界语言数据的细微差别，往往达到或甚至超越人类专家水平的能力（OpenAI，2023）。近来，研究人员已经表明，LLMs可能被扩展为在各种环境中作为代理，当与特定领域的知识和模块结合时，能够执行复杂的动作和任务（Xi等人，2023）。这些场景由复杂推理、理解代理的角色及其环境以及多步骤规划特征，测试了代理在其环境约束条件下做出高度细致和复杂决策的能力（Wu等人，2023；Meta Fundamental AI Research (FAIR) Diplomacy Team等人，2022）。

建立在这些初步努力的基础上，AI社区正处于一个重要的范式转变的前沿，从创建用于被动、结构化任务的AI模型转向能够在多样化和复杂环境中承担动态、具有代理性角色的模型。在这个背景下，本文调查了使用LLMs和VLMs作为代理的巨大潜力，强调那些具有语言能力、视觉认知、语境记忆、直觉推理和适应性融合的模型。利用LLMs和VLMs作为代理，特别是在游戏、机器人和医疗保健等领域的应用，不仅承诺提供一个严格评估最先进AI系统的平台，而且预示了以Agent为中心的AI将在整个社会和行业中带来的变革性影响。当完全利用时，具有代理性的模型可以重新定义人类经验并提升运营标准。这些模型带来的广泛自动化的潜力预示着在行业和社会经济动态中的巨大变革。这类进步将与多方面的排行榜交织在一起，不仅是技术性的，也是伦理性的，我们将在第11节中详述。我们深入探讨了这些Agent AI子领域的交叉区域，并在图1中展示它们的相互联系。

<p align="center">
  <img src="/assets/images/f1.png" alt="Overview">
</p>

## **1.2 Background**

现在我们将介绍支持Agent AI概念、理论背景和现代实现的相关研究论文。

大型基础模型：LLMs和VLMs一直在推动发展通用智能机器的努力（Bubeck等人，2023年；Mirchandani等人，2023年）。尽管它们是使用大型文本语料库训练的，但它们出色的解决问题能力不限于标准的语言处理领域。LLMs可以潜在地解决以前被认为是仅限于人类专家或领域特定算法的复杂任务，范围从数学推理（Imani等人，2023年；Wei等人，2022年；Zhu等人，2022年）到回答职业法律问题（Blair-Stanek等人，2023年；Choi等人，2023年；Nay, 2022年）。最近的研究表明，LLMs可以用来为机器人和游戏AI生成复杂的计划（Liang等人，2022年；Wang等人，2023a,b；Yao等人，2023a；Huang等人，2023a），这标志着LLMs作为通用智能代理的一个重要里程碑。

具身AI：许多工作利用LLMs进行任务规划（Huang等人，2022a；Wang等人，2023b；Yao等人，2023a；Li等人，2023a），特别是利用LLMs的WWW（万维网）规模的领域知识和零样本实体化（zero-shot embodied）能力来执行复杂的任务规划和推理。最近的机器人研究也利用LLMs进行任务规划（Ahn等人，2022a；Huang等人，2022b；Liang等人，2022），通过将自然语言指令分解成一系列子任务，子任务形式可以是自然语言形式，也可以是Python代码，然后使用低级控制器（low-level controller）来执行这些子任务。此外，它们还结合环境反馈来提高任务性能（Huang等人，2022b），（Liang等人，2022），（Wang等人，2023a），以及（Ikeuchi等人，2023）。

交互式学习：为交互式学习设计的AI代理使用机器学习技术与用户互动的结合运作。最初，AI代理是在大型数据集上训练的。这个数据集包含各种类型的信息，具体取决于代理的预定功能。例如，为语言任务设计的AI会在大规模文本数据语料库上训练。训练涉及使用机器学习算法，这些算法可能包括像神经网络这样的深度学习模型。这些训练模型使AI能够识别模式，做出预测，并基于其接受训练的数据生成回应。AI代理也可以从与用户的实时互动中学习。这种交互式学习可以通过多种方式进行：1）基于反馈的学习：AI根据直接的用户反馈调整其回应（Li等人，2023b；Yu等人，2023a；Parakh等人，2023；Zha等人，2023；Wake等人，2023a,b,c）。例如，如果用户纠正了AI的回应，AI可以使用这些信息来改进未来的回应（Zha等人，2023；Liu等人，2023a）。2）观察学习：AI观察用户互动并隐式学习。例如，如果用户经常提出类似的问题或以特定方式与AI互动，AI可能会调整其回应以更好地适应这些模式。这使得AI代理能够理解和处理人类语言、多模态环境、解释跨现实环境，并生成人类用户的回应。随着时间的推移，随着更多用户互动和反馈，AI代理的性能通常会不断提高。这个过程通常由人类操作员或开发者监督，他们确保AI适当学习并且没有发展出偏见或错误的模式。

## **1.3 Overview**

多模态Agent AI（MAA）是一组系统，能够基于对多模态感官输入的理解，在给定环境中生成有效行动。随着大型语言模型（LLM）和视觉-语言模型（VLM）的出现，已经有许多不同领域的MAA系统被提出，从基础研究到应用领域都有所涉及。虽然这些研究领域通过与各自领域的传统技术（例如，视觉问题回答和视觉-语言导航）的整合而迅速发展，但它们共享一些共同的兴趣点，如数据收集、基准测试和伦理观点。在本文中，我们专注于一些代表性的MAA研究领域，包括多模态性、游戏（VR/AR/MR）、机器人技术和健康护理，并力图提供这些领域中公共关注点的综合知识。因此，我们希望可以学习到MAA的基本原理，并获得进一步推动研究的洞见。具体的学习成果包括：

- MAA概览：深入了解其原理及其在现代应用中的角色，为研究人员提供关于其重要性和用途的深入理解。
• **方法论**：通过在游戏、机器人技术和健康护理领域的案例研究，详细举例说明LLM和VLM是如何增强MAA的。
• **性能评估**：在相关数据集上对MAA进行评估的指南，专注于它们的有效性和泛化能力。
• **伦理考虑**：讨论部署Agent AI的社会影响和伦理领导榜，强调负责任的发展实践。
• **新兴趋势和未来领导榜**：对各领域的最新发展进行分类，并讨论未来的方向。

基于计算机的行动Agent和通用Agent（GA）非常适用于许多任务。一个GA要真正对用户有价值，它可以自然地进行交互，并泛化到广泛的上下文和模态。我们的目标是培养一个充满活力的研究生态系统，并在Agent AI社区中创造共同的身份感和目标感。MAA有潜力在多种情境和模态中广泛适用，包括来自人类的输入。因此，我们相信这个Agent AI领域可以吸引多种背景的研究者，促进一个充满活力的Agent AI社区和共享的目标。在学术界和工业界尊敬的专家的带领下，我们期望这篇论文将是一次互动丰富的体验，包含agent指导、案例研究、任务会话和实验讨论，确保研究人员有一个全面且参与感强的学习体验。

本文旨在就Agent AI领域的当前研究提供一般性而综合性的知识。为此，本文的其他部分安排如下。第2节概述了Agent AI如何受益于与相关新兴技术的整合，尤其是大型基础模型。第3节描述了我们提出的用于训练Agent AI的新范式和框架。第4节提供了Agent AI训练中广泛使用的方法论概览。第5节分类讨论了各种类型的agent。第6节介绍了在游戏、机器人技术和健康护理中应用Agent AI的情况。第7节探讨了研究社区努力开发多功能Agent AI的成果，这种AI能够适用于各种模态、领域，并且能够跨越虚拟到现实的鸿沟。第8节讨论了不仅仅依赖于预训练的基础模型，而且通过与环境和用户的互动不断学习和自我提高的Agent AI的潜力。第9节介绍了我们为训练多模态Agent AI而设计的新数据集。第11节讨论了AI agent伦理考量的热门话题、局限性以及我们论文的社会影响。

# 2 Agent AI Integrtion

根据黄等人（2023a）和曾等人（2023）的前期研究，基于LLM和VLM的基础模型在体现AI领域仍展现出有限的性能，特别是在理解、生成、编辑以及与未见环境或场景交互方面。因此，这些限制导致AI Agent输出效果不佳。当前以Agent为中心的AI模型化方法重点关注直接可获取并且明确定义的数据（例如世界状态的文本或字符串表示）并且通常使用从大规模预训练中学到的领域与环境独立的模式，来预测每个环境的行动输出（例如，奚等人，2023；王等人，2023c；龚等人，2023a；吴等人，2023）。在黄等人（2023a）中，我们调查了结合大型基础模型的知识引导下的协作和互动场景生成任务，并展示了令人鼓舞的结果，这表明知识基础的LLM Agent能够提升2D和3D场景理解、生成和编辑的性能，以及与其他人类-Agent交互（黄等人，2023a）。通过整合Agent AI框架，大型基础模型能够更深入地理解用户输入，形成一个复杂而适应性强的HCI系统。在生成AI、体现AI、多模型学习的知识增强、混合现实生成、文本到视觉编辑、2D/3D仿真游戏或机器人任务中的人类交互等方面，LLM和VLM的新兴能力发挥着不可见的作用。Agent AI在基础模型的最新进展表明，这是解锁体现Agent中的通用智能的迫切催化剂。大型行为模型，或Agent视觉语言模型，为通用的体现系统如规划、问题解决以及在复杂环境中的学习开辟了新的可能性。Agent AI在元宇宙中进一步测试，并规划了早期版本的AGI的道路。

## **Infinite AI agent**

AI Agent拥有根据其训练和输入数据解释、预测和响应的能力。虽然这些能力是先进的并且不断改进的，但认识到它们的局限性以及它们所训练的底层数据的影响是很重要的。AI Agent系统通常具备以下能力：

1. **预测建模**：AI Agent能够根据历史数据和趋势预测可能的结果或建议下一步行动。例如，它们可能会预测文本的延续、问题的答案、机器人的下一个动作或情境的解决方案。
2. **决策制定**：在某些应用中，AI Agent可以基于其推理作出决策。通常，Agent会根据最有可能实现特定目标的方案来做出决策。对于像推荐系统这样的AI应用，Agent可以根据其对用户偏好的推断来决定推荐哪些产品或内容。
3. **处理模糊性**：AI Agent通常可以通过基于上下文和训练来推断最可能的解释来处理模糊的输入。然而，它们这样做的能力受到它们训练数据和算法范围的限制。
4. **持续改进**：尽管一些AI Agent有能力从新数据和互动中学习，但是许多大型语言模型在训练之后不会持续更新它们的知识库或内部表征。它们的推断通常完全基于最后一次训练更新时可用的数据。

我们在图2中展示了具有出现机制的增强交互Agent，这些Agent适用于多模态和跨现实不可知的集成。AI Agent需要针对每项新任务收集大量的训练数据，这对许多领域来说可能既昂贵又不可行。在这项研究中，我们开发了一个能够从一般基础模型（例如，GPT-X, DALL-E）转移记忆信息到新领域或场景的无限Agent，用于物理或虚拟世界中的场景理解、生成和交互式编辑。

<p align="center">
  <img src="/assets/images/f2.png" alt="multi-model agent">
</p>

在机器人技术中应用这种无限Agent的一个实例是RoboGen（王等人，2023d）。在这项研究中，作者提出了一个自动运行任务提议、环境生成和技能学习循环的流程。RoboGen是将大型模型中嵌入的知识转移到机器人技术的一次尝试。

## 2.2 **Agent AI with Large Foundation Models**

近期的研究表明，大型基础模型在创建用作确定在环境限制下Agent行动的基准数据方面发挥着至关重要的作用。例如，使用基础模型进行机器人操控（Black等人，2023；Ko等人，2023）和导航（Shah等人，2023a；周等人，2023a）。举例来说，Black等人利用图像编辑模型作为一个高层规划器，生成未来子目标的图像，从而指导低层政策（Black等人，2023）。对于机器人导航，Shah等人提出了一个系统，使用LLM识别文本中的地标，以及利用VLM将这些地标与视觉输入关联起来，通过自然语言指令加强导航（Shah等人，2023a）。

同时，对于根据语言和环境因素生成条件化人类动作的研究兴趣也在增长。已经提出了多个AI系统来生成针对特定语言指令量身定制的动作（金等人，2023；张等人，2022；Tevet等人，2022）以及适应各种3D场景（王等人，2022a）。这一系列研究强调了生成模型在增强AI Agent适应性和响应性方面的日益增长的能力。

### **2.2.1 Hallucinations**

生成文本的Agent常常容易出现“hallucinations（幻觉）”，即生成的文本没有意义或对所提供的源内容不忠实（Raunak等人，2021；Maynez等人，2020）。幻觉可以分为两类，内在的和外在的（姬等人，2023）。内在幻觉是与源材料相矛盾的幻觉，而外在幻觉则是生成的文本包含了源材料原本未包含的额外信息。

为了减少语言生成中幻觉的发生率，一些有希望的方法包括使用检索增强生成（Lewis等人，2020；Shuster等人，2021）或通过外部知识检索来实现自然语言输出的其他方法（Dziri等人，2021；Peng等人，2023）。一般来说，这些方法试图通过检索额外的源材料，并提供机制来检查生成回应和源材料之间是否存在矛盾来增强语言生成。

在多模态Agent系统的背景下，VLMs也被发现会产生幻觉（周等人，2023b）。视觉基础的语言生成产生幻觉的一个常见原因是由于过分依赖训练数据中对象和视觉线索的共现（Rohrbach等人，2018）。那些完全依赖于预先训练的LLMs或VLMs并使用有限的环境特定微调的AI Agent可能特别容易产生幻觉，因为它们依赖于预训练模型的内部知识库来生成行为，并且可能无法准确理解它们被部署的世界状态的动态。

### **2.2.2 Biases and Inclusivity**

基于LLMs或LMMs（大型多模态模型）的AI Agent由于其设计和训练过程中固有的几个因素而存在偏见。当设计这些AI Agent时，我们必须注意包容性，并了解所有最终用户和利益相关者的需求。在AI Agent的背景下，包容性指的是采取的措施和原则，确保Agent的回应和互动包容、尊敬并对不同背景的广泛用户群体敏感。我们在下方列举了Agent偏见和包容性的关键方面。

- **训练数据**：基础模型是在从互联网收集的大量文本数据（包括书籍、文章、网站和其他文本来源）上训练的。这些数据通常反映了人类社会中存在的偏见，模型可能无意中学习并复制这些偏见。这包括与种族、性别、族裔、宗教和其他个人属性相关的刻板印象、偏见和倾向性观点。特别是通过训练网络数据并且经常只是英文文本，模型隐性地学习到了西方教育工业化富裕民主（WEIRD）社会的文化规范，这些社会在互联网上占有不成比例的大量存在。然而，至关重要的是要认识到，由人类创建的数据集不能完全没有偏见，因为它们通常反映了社会的偏见和最初生成和/或编译数据的个人的倾向。
- **历史和文化偏见**：AI模型是在来自多种内容的大型数据集上训练的。因此，训练数据通常包括来自各种文化的历史文本或材料。特别是，来自历史来源的训练数据可能包含表示特定社会文化规范、态度和偏见的攻击性或贬低性语言。这可能导致模型持续过时的刻板印象，或者不完全理解当代文化转变和细微差别。
- **语言和语境限制**：语言模型可能在理解和准确表达语言的微妙之处（如讽刺、幽默或文化参考）方面存在困难。这可能导致在某些语境中出现误解或偏见的回应。此外，有许多口头语言的方面不被纯文本数据捕捉，导致人类对语言的理解和模型对语言的理解之间可能存在断层。
- **政策和指南**：AI Agent在严格的政策和指南下运行，以确保公平与包容。例如，在生成图像时，有规则多样化地描绘人物，避免与种族、性别和其他属性相关的刻板印象。
- **过度概括**：这些模型往往根据训练数据中看到的模式生成回应。这可能导致过度概括，模型可能产生似乎对某些群体做出刻板印象或广泛假设的回应。
- **持续监控和更新**：AI系统不断被监控和更新，以应对任何新出现的偏见或包容性问题。用户反馈和AI伦理研究的持续进展在这个过程中扮演着关键角色。

尽管有这些措施，AI Agent仍然表现出偏见。Agent AI研究和开发的持续努力聚焦于进一步减少这些偏见，并增强Agent AI系统的包容性和公平性。减轻偏见的努力包括：

- **多样化和包容性训练数据**：努力包括更多样化和包容性的来源范围在训练数据中。
- 检测和纠正偏见**：当前的研究关注于检测和纠正模型回应中的偏见。
- **伦理指南和政策**：模型经常受到旨在减轻偏见并确保尊重、包容互动的伦理指南和政策的约束。
- **多样性表达**：确保AI Agent生成的内容或提供的回应代表广泛的人类经历、文化、族裔和身份。这在图像生成或叙事构建等场景中尤其相关。
- **减轻偏见**：积极工作以减少AI的回应中的偏见。这包括与种族、性别、年龄、残疾、性取向和其他个人特征相关的偏见。目标是提供公平、均衡的回应，不会延续刻板印象或偏见。
- **文化敏感性**：设计AI时要具有文化敏感性，承认并尊重文化规范、实践和价值观的多样性。这包括理解并适当响应文化参考和细微差别。
- **可访问性**：确保AI Agent对不同能力的用户，包括残疾用户，都是可访问的。这可能涉及加入使视觉、听觉、运动或认知障碍人士更容易互动的特性。
- **语言包容性**：为了满足全球用户基础，提供多种语言和方言的支持，对一种语言内的微妙差异和变化保持敏感（刘等人，2023b）。
- **伦理和尊重互动**：Agent被编程以与所有用户进行伦理和尊重的互动，避免产生可能被视为攻击性、有害或不尊重的回应。
- **用户反馈和适应**：吸纳用户反馈以不断提升AI Agent的包容性和有效性。这包括从互动中学习，以更好地理解和服务于多样化的用户群体。
- **符合包容性指南的合规性**：遵循AI Agent中包容性的既定指南和标准，这些通常由行业团体、伦理委员会或监管机构设定。

尽管有这些努力，重要的是要意识到在回应中偏见的可能性，并用批判性思维来解读它们。AI Agent技术和伦理实践的持续改进旨在随时间减少这些偏见。Agent AI包容性的总体目标之一是创造一个对所有用户都尊重和可访问的Agent，不论他们的背景或身份如何。

### **2.2.3 Data Privacy and Usage**

AI代理的一个关键伦理考虑因素包括理解这些系统如何处理、存储以及可能检索用户数据。我们在下面讨论了一些关键方面：

**数据收集、使用和目的**。当使用用户数据来提升模型表现时，模型开发者可以获取Agent在生产环境下与用户互动时收集的数据。一些系统允许用户通过账户查看他们的数据，或者向服务提供商提出请求。重要的是要认识到代理在这些互动中收集了什么数据。这可能包括文本输入、用户使用模式、个人偏好，有时还包括更敏感的个人信息。用户还应该了解从他们的互动中收集到的数据是如何被使用的。如果由于某种原因，AI持有关于特定人或群体的不正确信息，一旦识别出来，应该有一个机制允许用户帮助纠正。这对于准确性和尊重所有用户和群体都是很重要的。获取和分析用户数据的常见用途包括改善用户互动、个性化回应和系统优化。开发者必须确保数据不被用于用户未同意的目的，比如未经请求的市场营销。

**存储与安全**。开发者应该知道用户互动数据存储在哪里，以及为防止未经授权的访问或数据泄露而采取了哪些安全措施。这包括加密，安全服务器和数据保护协议。特别重要的是确定Agent数据是否与第三方共享，以及在何种条件下共享。这应该是透明的，并通常需要用户同意。

**数据删除与保留**。对于用户来说，了解用户数据存储多久以及如何请求删除也很重要。许多数据保护法规赋予用户被遗忘的权利，这意味着他们可以要求删除他们的数据。AI代理必须遵守数据保护法规，如欧盟的GDPR或加利福尼亚州的CCPA。这些法律规范了数据处理实践和用户关于其个人数据的权利。

**数据可移植性和隐私政策**。此外，开发者必须创建代理的隐私政策，以向用户文档化并解释其数据的处理方式。这应详细说明数据的收集、使用、存储和用户权利。开发者应确保他们获得用户对数据收集的同意，特别是对于敏感信息。用户通常有选择退出或限制他们提供的数据的选项。在某些司法管辖区，用户甚至可能有权要求以一种可以转移到另一个服务提供商的格式获取他们的数据副本。

**匿名化**。对于用于更广泛分析或AI培训的数据，理想情况下应该进行匿名处理以保护个人身份。开发者必须了解自己的代理在互动中如何检索和使用历史用户数据。这可能是出于个性化或提高回应相关性的目的。

总之，了解AI代理的数据隐私涉及到意识到用户数据是如何被收集、使用、存储和保护的，以及确保用户了解他们关于访问、纠正和删除其数据的权利。了解用户和代理检索数据的机制也对于全面理解数据隐私至关重要。

### **2.2.4 Interpretability and Explainability**

**模仿学习 → 解耦**。在强化学习（RL）或模仿学习（IL）中，Agent通常通过一个连续的反馈循环进行训练，从一个随机初始化的策略开始。然而，这种方法面临着在陌生环境中获取初始奖励的障碍，特别是当奖励稀缺或只在长步骤交互的末端可用时。因此，一个更优的解决方案是使用通过IL训练的具有无限记忆的Agent，它可以从专家数据中学习策略，如图3所示，通过紧急基础设施探索和利用未见过的环境空间。有了专家特性的帮助，Agent可以更好地探索和利用未见过的环境空间。Agent AI能够直接从专家数据中学习策略和新范式流程。

传统的IL中，Agent通过模仿专家演示者的行为来学习策略。然而，直接学习专家策略并不总是最佳方法，因为Agent可能无法很好地泛化到未见过的情况。为了解决这个问题，我们提出了一种通过上下文提示或隐式奖励函数学习Agent的方法，该奖励函数捕捉专家行为的主要方面，如图3所示。这为具有无限记忆的Agent提供了物理世界行为数据，以完成任务执行，从专家演示中学习而来。它有助于克服现有的模仿学习缺点，例如需要大量的专家数据和复杂任务中的潜在错误。Agent AI背后的关键思想分为两部分：1) 收集物理世界专家演示为状态-行为对的无限Agent；和2) 模仿Agent生成器的虚拟环境。模仿Agent产生模仿专家行为的动作，而Agent通过减少专家行为与由学习策略生成的行为之间差异的损失函数来学习从状态到行为的策略映射。

**解耦 → 泛化**。而非依赖于特定任务的奖励函数，Agent从专家演示中学习，这些演示提供了涵盖各种任务方面的多样化状态-行为对。然后，Agent通过模仿专家的行为学习一个将状态映射到行为的策略。在模仿学习中，解耦是指将学习过程与特定任务的奖励功能分离，允许策略在不显性依赖于特定任务的奖励函数的情况下泛化到不同任务。通过解耦，Agent能够从专家演示中学习并学会一个适应多种情境的策略。解耦实现了迁移学习，即在一个领域中学习的策略可以通过最小的微调适应其他领域。通过学习一个不绑定于特定奖励函数的通用策略，Agent可以利用它在一个任务中获得的知识在其他相关任务中表现得很好。由于Agent不依赖特定的奖励函数，它可以适应奖励函数或环境的变化，而无需大量重新训练。这使得学到的策略更加健壮，并且能够泛化到不同的环境中去。在此背景下，解耦是指学习过程中两个任务的分离：学习奖励函数和学习最优策略。

**泛化 → 新颖行为**。泛化解释了怎样从更简单的组件或规则中产生出新颖属性或行为。关键思想在于识别控制系统行为的基本元素或规则，例如个别的神经元或基础算法。随后，观察这些简单组件或规则是如何相互作用的。这些组件的相互作用往往导致复杂行为的出现，这是不能通过单独检查个别组件来预测的。泛化跨不同复杂性等级允许一个系统学习适用于这些层面的通用原则，导致新颖属性的出现。这使得系统能够适应新情况，展示了从更简单规则中出现更复杂行为的能力。而且，跨不同复杂度等级的泛化能力有助于从一个领域到另一个领域的知识迁移，随着系统的适应，这有助于在新的背景下复杂行为的出现。

<p align="center">
  <img src="/assets/images/f3.png" alt="Emergent Interactive Mechanism">
</p>

### **2.2.5 Inference Augmentation**

人工智能Agent的推理能力在于其解释、预测以及基于其训练和输入数据作出响应的能力。虽然这些能力颇为先进并且不断得到改善，但认识到它们的局限性以及它们所依赖的底层数据的影响是很重要的。特别是在大型语言模型（LLM）的背景下，这指的是其根据训练数据和收到的输入信息来得出结论、做出预测和生成响应的能力。在AI Agent中增强推理，指的是通过附加工具、技术或数据来增强AI的天然推理能力，以提高其性能、准确性和实用性。这在复杂决策场景或处理复杂或专业内容时尤为重要。我们在下面特别指出了推理增强的几个重要来源：

**数据丰富**。引入附加的、通常是外部的数据源，以提供更多的背景或上下文可以帮助AI Agent做出更为明智的推理，特别是在其训练数据可能有限的领域。例如，AI Agent可以从对话或文本的上下文中推断意义。它们分析给定的信息并使用它来理解用户查询的意图和相关细节。这些模型擅长识别数据中的模式，并利用在训练期间学到的模式，对语言、用户行为或其他相关现象作出推理。

**算法优化**。改进AI的底层算法，以便做出更好的推理。这可能涉及使用更先进的机器学习模型、整合不同类型的AI（如结合自然语言处理与图像识别），或更新算法以更好地处理复杂任务。在语言模型中的推理包括理解和生成人类语言。这包括理解语调、意图和不同语言构造的微妙之处。

**人在循环中（Human-in-the-Loop, HITL）**。在AI的推理中加入人类输入可以在需要人类判断的领域特别有用，如伦理考量、创造性任务或模糊情境中。人类可以提供指导、纠正错误或提供Agent无法自行推断的见解。

**实时反馈集成**。使用用户或环境的实时反馈来增强推理是另一个提高推理性能的有前途方法。例如，AI可能会根据实时用户反馈或在动态系统中的变化条件调整其建议。或者，如果Agent在模拟环境中的行为违反了某些规则，可以动态给予反馈以帮助其自行纠正。

**跨域知识迁移**。利用来自一个领域的知识或模型来改进另一个领域的推理可能特别有帮助，尤其是在专业领域内生成输出时。例如，为语言翻译开发的技术可能被应用到代码生成上，或者医学诊断的见解可能增强机械的预测性维护。

**针对特定用例的定制**。为特定应用或行业定制AI的推理能力可能涉及对AI进行专业数据集的训练或微调其模型，以更好地适应特定任务，如法律分析、医疗诊断或财务预测。由于一个领域的特定语言或信息可能与其他领域的语言大相径庭，所以在领域特定信息上微调Agent可能是有益的。

**伦理和偏见考量**。确保增强过程不引入新的偏见或伦理问题是非常重要的。这涉及到仔细考虑附加数据来源或新的推理增强算法对公平和透明度的影响。在进行推理时，特别是关于敏感话题时，AI Agents有时需要考虑伦理问题。这包括避免有害的刻板印象、尊重隐私以及确保公平。

**持续学习和适应**。定期更新和完善AI的能力，以跟上新的发展、变化的数据景观以及不断演变的用户需求。

总结来说，AI Agent中的推理增强涉及一系列方法，通过这些方法可以通过额外数据、改进的算法、人类输入和其他技术来增强其自然的推理能力。根据具体的使用情况，这种增强通常对处理复杂任务和确保Agent输出的准确性至关重要。

### **2.2.6 Regulation**

近期，Agent AI在技术上取得了显著的进步，它的集成到具身系统中为我们与代理进行更具沉浸感、动态化和富有吸引力的交互体验打开了新的可能性。为了加快进度并简化Agent AI开发中的繁琐工作，我们提议开发下一代AI赋能的代理交互流程。构建一个人机协作系统，使人类与机器能够进行有意义的通信和交互。该系统可以利用LLM或VLM的对话能力和广泛的动作来与人类玩家交流，识别人类的需求。然后应玩家请求执行适当的动作进行帮助。

在利用LLM/VLM构建人机协作系统时，需要注意的一点是，这些系统作为“黑箱”运行，会产生不可预测的输出。这种不确定性在物理设置中，比如操作实际的机器人，可能变得至关重要。应对这一挑战的一种方法是通过提示工程来限定LLM/VLM的关注焦点。例如，在根据指令进行机器人任务规划时，将环境信息纳入提示已经被报道能比仅依靠文本生成更稳定的输出(Gramopadhye和Szafir, 2022)。这份报告得到了明斯基的AI框架理论(Minsky, 1975)的支持，该理论认为LLM/VLM所需解决的问题空间是由给定的提示定义的。另一种方法是设计提示，使LLM/VLM包含解释性文本，以便用户理解模型关注或识别了什么。此外，实施一个允许在人类指导下进行预执行验证和修改的更高层次，可以促进在这种指导下工作的系统的操作（见图4）。

<p align="center">
  <img src="/assets/images/f4.png" alt="robot teaching system">
</p>

## **2.3 Agent AI for Emergent Abilities**

在互动Agent AI系统的使用渐趔普及之际，大多数提出的方法仍然面临在未见过的环境或场景中泛化性能的挑战。目前的建模实践要求开发人员为每个领域准备大量的数据集以进行微调/预训练模型；然而，这个过程既昂贵又在新领域中实际上不可能实现。为了解决这个问题，我们构建了能够利用通用基础模型的知识记忆（比如ChatGPT、Dall-E、GPT-4等）的互动Agent，这些Agent专为在人类与Agent之间创建协作空间的新情景而设计。我们发现了一种新出现的机制——我们将其命名为知识推理交互的混合现实（Mixed Reality with Knowledge Inference Interaction）——这种机制便于与人类合作解决复杂真实世界环境中的挑战性任务，并且使得Agent能够探索未知环境，以适应虚拟现实。

对这种机制，Agent学习以下两个方面：一是跨模态的微反应：从显式的网络资源中收集每一次互动任务（例如理解未见过的场景）的相关个体知识，并通过隐式地从预训练模型的输出中推断；二是现实不可知的宏观行为：改进语言和多模态领域中互动维度和模式，并基于特定角色，某个目标变量，以及混合现实与LLM中协作信息的多元化影响进行调整。

我们调查了以知识为导向的互动协同效应任务，以及结合使用各种OpenAI模型进行合作场景生成的任务，并展示了互动Agent系统如何进一步提升我们设置中的大型基础模型的有希望的结果。它整合并提高了复杂自适应AI系统的泛化深度、意识和可解释性。

<p align="center">
  <img src="/assets/images/f5.png" alt="new agent paradigm for a multi-modal generalist agent.">
</p>

# 3 **Agent AI Paradigm**

在本节中，我们将讨论一个新的Agent AI训练范式和框架。我们希望通过我们提出的框架实现几个目标：

- 利用现有的预训练模型和预训练策略，有效地为我们的Agent引导具有理解重要模态的能力，例如文本或视觉输入。
- 支持足够的长期任务规划能力。
- 融入一个允许学习知识被编码和稍后检索的记忆框架。
- 允许使用环境反馈来有效训练Agent学习采取哪些行动。

我们在图5中展示了一个高层级的新Agent框图，勾勒出了这样一个系统的重要子模块。

## **3.1 LLMs and VLMs**

我们可以使用LLM或VLM模型来引导Agent的组成部分，如图5所示。特别是，LLM已被证明在任务规划方面表现良好（Gong et al., 2023a），包含重要的世界知识（Yu et al., 2023b），并且展示出令人印象深刻的逻辑推理能力（Creswell et al., 2022）。此外，像CLIP（Radford et al., 2021）这样的VLM提供了一个与语言对齐的通用视觉编码器，以及提供零样本视觉识别能力。例如，使用冻结的CLIP模型作为视觉编码器的最先进的开源多模态模型，如LLaVA（Liu et al., 2023c）和InstructBLIP（Dai et al., 2023）。

## **3.2 Agent Transformer Definition**

与其使用固定的LLM和VLM作为AI Agent的组成部分，我们也可以使用一个single-agent transformer模型来处理视觉标记和语言标记作为输入，类似于Gato（Reed et al., 2022）。除了视觉和语言之外，我们还增加了第三种通用类型的输入，我们将其称为agent tokens。从概念上讲，Agent tokens被用来为模型的输入和输出空间保留一个特定的子空间，用于agent行为。对于机器人或游戏玩家来讲，这可能表现为控制器的输入动作空间。当训练Agent使用特定工具，例如图像生成或图像编辑模型，或用于其他API调用时，Agent token也可以被使用。如图7所示，我们可以将Agent token与视觉和语言标记结合起来，生成一个统一的界面来训练多模态Agent AI。与使用大型私有LLM作为Agent相比，使用agent transformer有几个优点。首先，模型可以很容易地定制到非常具体的agent任务，这些任务可能很难用自然语言表示（例如，控制器输入或其他特定动作）。因此，Agent可以通过环境互动和领域特定数据来提高性能。其次，通过访问Agent token的概率，我们可以更容易理解模型为什么会或不会采取特定动作。第三，像医疗和法律这样的领域有严格的数据隐私要求。最后，一个相对较小的agent transformer可能比一个较大的私有LLM要便宜得多。

<p align="center">
  <img src="/assets/images/f6.png" alt="old paradigm">
</p>

<p align="center">
  <img src="/assets/images/f7.png" alt="new paradigm">
</p>

## 3.3 **Agent Transformer Creation**

如上图5所示，我们可以使用结合了LLM和VLM启动的Agent范式，同时利用从大型基础模型中生成的数据来训练Agent Transformer模型，以学习执行特定目标。在这个过程中，Agent模型被训练成为专门针对特定任务和领域的定制模型。这种方法允许你利用现有的基础模型所学习的特征和知识。我们在下面通过两个步骤简化地概述了这个过程：

1. 定义领域内的目标。为了训练Agent Transformer，需要清晰地定义Agent在每个特定环境中的目标和行动空间。这包括确定Agent需要执行的特定任务或行动，并为每个任务或行动分配独特的Agent token。此外，任何可以用来识别任务成功完成的自动规则或程序都可以显著提高用于训练的数据量。否则，需要使用基础模型生成的或人工注释的数据来训练模型。收集数据并能够评估Agent的表现之后，就可以开始持续改进的过程。
2. 持续改进。持续监控模型的性能和收集反馈是过程中的关键步骤。应该使用反馈来进行进一步的微调和更新。同时，确保模型不会延续偏见或不道德的结果也是至关重要的。这需要仔细检查训练数据，定期检查输出中的偏见，并在需要时培训模型以识别和避免偏见。一旦模型达到令人满意的表现，便可以部署用于预定的应用。持续监控对于确保模型按预期工作并进行必要调整至关重要。有关这个过程的更多细节、训练数据的来源以及有关Agent AI持续学习的详细信息，请参阅第8节。

# 4 **Agent AI Learning**

## **4.1 Strategy and Mechanism**

交互式AI的策略应用于不同的领域，它扩展了通过训练有素的Agent来调用大型基础模型的范式；这样的Agent积极寻求收集用户反馈、行动信息、用于生成和交互的有用知识。有时候，并不需要对LLM/VLM模型重新进行训练，而是在测试时通过提供改进的上下文提示来提升Agent的性能。另一方面，它总是涉及到一个知识/推理/常识/推断的交互式建模，这种建模是通过三重系统的结合来完成的——第一个系统负责从多模型查询中检索知识，第二个系统负责从相关Agent进行交互式生成，最后一个则是训练新的、信息量大的自监督训练，或者用增强学习或模仿学习在改进的方式下进行预训练。

### **4.1.1 Reinforcement Learning (RL)**

利用强化学习（RL）来训练展现智能行为的交互式Agent有着悠久的历史。RL是一种基于奖励（或惩罚）来学习状态和行动之间最优关系的方法论，这种奖惩是作为它行动的结果而得到的。强化学习是一个高度可扩展的框架，已经被应用于包括机器人学在内的众多应用中，然而，它通常面临着若干挑战，而LLMs/VLMs已经显示出它们缓解或克服这些困难的潜力：

- 奖励设计：策略学习的效率在很大程度上取决于奖励函数的设计。设计奖励函数不仅需要了解强化学习算法，还需要深刻理解任务的本质，因此通常需要基于专家经验来构建函数。有几项研究探讨了使用LLMs/VLMs来设计奖励函数的方法（Yu et al., 2023a; Katara et al., 2023; Ma et al., 2023）。
- 数据收集与效率：鉴于其探索性质，基于RL的策略学习需要大量的数据（Padalkar et al., 2023）。当策略涉及管理长序列或整合复杂行动时，对海量数据的需求变得尤为明显。这是因为这些情况要求更细致的决策制定，并且从更广泛的情形中学习。近期的研究努力提高数据生成，以支持策略学习（Kumar et al., 2023; Du et al., 2023）。此外，在一些研究中，这些模型被集成到奖励函数中，以改进策略学习（Sontakke et al., 2023）。与此同时，另一方面的研究专注于使用VLMs（Tang et al., 2023; Li et al., 2023d）和LLMs（Shi et al., 2023）在学习过程中实现参数效率。
- 长期规划步骤：与数据效率的问题相关，当行动序列的长度增加时，RL变得更具挑战性。这是由于行动与奖励之间关系的模糊性，即所谓的信用分配问题，以及需要探索的状态数量增加，需要大量的时间和数据。解决长期和复杂任务的一种典型方法是将任务分解为一系列子目标，并应用预训练的策略来解决每个子目标（例如，（Takamatsu et al., 2022））。这一思路属于任务与运动规划（TAMP）框架（Garrett et al., 2021）。TAMP由两个主要部分组成：任务规划，即识别高级行动的序列；以及运动规划，即找到物理一致的、无碰撞的轨迹以实现任务计划的目标。

LLMs非常适合TAMP，而且最近的研究常常采取一种方法，即使用LLMs来执行高层次的任务规划，而低层次的控制则使用基于RL的策略来解决（Xu et al., 2023; Sun et al., 2023a; Li et al., 2023b; Parakh et al., 2023）。LLMs的高级能力使它们能够有效地将抽象指令分解成子目标（Wake et al., 2023c），这有助于增强机器人系统中的语言理解能力。

### **4.1.2 Imitation Learning (IL)**

强化学习（RL）旨在通过与环境的交互行为、探索性行为以及最大化奖励来训练策略，而模仿学习（IL）则寻求利用专家数据来模拟有经验的Agent或专家的行为。例如，在机器人技术中，基于IL的一个主要框架就是行为克隆（BC）。BC是一种方法，通过直接复制来训练机器人模仿专家的动作。在这种方式下，记录下专家在执行特定任务时的动作，然后训练机器人在类似情境下复制这些动作。基于BC的近期方法经常集成了来自LLM/VLM的技术，使得更高级的端到端模型成为可能。例如，Brohan等人提出了RT-1（Brohan et al., 2022年）和RT-2（Brohan et al., 2023年）这两种基于变换器的模型，这些模型可以输出一个动作序列，用于基座和机械臂，输入是一系列的图像和语言。据报道，这些模型因为在大量训练数据上进行训练，展现出高度的泛化性能。

### **4.1.3 Traditional RGB**

利用图像输入来学习智能Agent的行为多年来一直是人们感兴趣的话题（Mnih等人，2015年）。使用RGB输入的固有挑战之一是维度的诅咒。为了解决这个问题，研究者们要么使用更多数据（Jang等人，2022年；Ha等人，2023年），要么在模型设计中引入归纳偏置来提高样本效率。特别是，作者们在模型结构中整合了3D结构，用于操控任务（Zeng等人，2021年；Shridhar等人，2023年；Goyal等人，2023年；James和Davison，2022年）。对于机器人导航，一些作者（Chaplot等人，2020a，b）利用地图作为一种表示方式。地图可以是通过聚合所有先前RGB输入的神经网络学习获得的，或者通过3D重建方法例如神经辐射场（Rosinol等人，2022年）来获得。

为了获取更多数据，研究者们使用图形模拟器合成合成数据（Mu等人，2021年；Gong等人，2023b），并尝试缩小仿真到现实世界（sim2real）的差距（Tobin等人，2017年；Sadeghi和Levine，2016年；Peng等人，2018年）。近期，一些集体努力致力于策划大规模数据集，旨在解决数据稀缺问题（Padalkar等人，2023年；Brohan等人，2023年）。另一方面，为了改善样本复杂性，也广泛研究了数据增强技术（Zeng等人，2021年；Rao等人，2020年；Haarnoja等人，2023年；Lifshitz等人，2023年）。

### **4.1.4 In-context Learning**

随着像GPT-3这样的大型语言模型（LLM）的出现，In-context learning已经被证明是解决自然语言处理(NLP)任务的一种有效方法（Brown等，2020年；Min等，2022年）。在NLP的多种任务中，通过在LLM提示(context)中提供任务示例，Few-shot prompts被发现是一种有效的方法来对模型输出进行上下文化。例如示例的多样性和展示的示例质量等因素可能提高模型输出的质量（An等，2023年；Dong等，2022年）。在多模态基础模型的背景下，像Flamingo和BLIP-2这样的模型（Alayrac等，2022年; Li等，2023c年）在只给出少数示例时已被证明能有效完成多种视觉理解任务。通过在特定动作被执行时纳入特定环境的反馈，可以进一步改进Agent在环境中的In-context learning（Gong等，2023a年）。

### **4.1.5 Optimization in the Agent System**

Agent系统的优化可以分为空间和时间两个方面。空间优化考虑Agent在物理空间内如何操作以执行任务。这包括了机器人间的协调、资源分配和保持有序的空间。

为了有效地优化Agent的AI系统，特别是大量Agent并行行动的系统，先前的工作集中于使用大批量强化学习（Shacklett等，2023年）。因为针对特定任务的多Agent交互数据集非常稀少，自我对弈的强化学习使得Agent团队能够随时间改进。但这也可能导致产生非常脆弱的Agent，它们只能在自我对弈中工作，而无法与人类或其他独立Agent共同作业，因为它们过度拟合了自我对弈的训练范式。为了解决这个问题，我们可以发现一组多样化的约定（Cui等，2023年；Sarkar等，2023年），并训练一个意识到广泛约定的Agent。基础模型可以进一步帮助与人类或其他独立Agents建立约定，使得与新Agents的协调更加顺畅。

另一方面，时间优化关注Agent随着时间的推移如何执行任务。这包括任务调度、顺序安排和时间线效率。例如，优化机器人手臂的轨迹是高效优化连续任务之间运动的一个例子（Zhou等，2023c年）。在任务调度层面，已经提出了如LLM-DP（Dagan等，2023年）和ReAct（Yao等，2023a年）的方法，通过交互地融合环境因素来解决高效的任务规划。

## **4.2 Agent Systems (zero-shot and few-shot level)**

### **4.2.1 Agent Modules**

我们对Agent范式的探索包括使用LLM或VLM开发交互式多模态Agent的Agent AI“模块”。我们的初始Agent模块便于训练或In-context learning，并采用了极简设计，以展示Agent有效安排和协调的能力。我们还探索了初步的基于提示的记忆技术，这些技术有助于更好的规划，并为领域内未来的行动方法提供信息。举例来说，我们的“MindAgent”基础设施包括5个主要模块：1）环境感知与任务规划，2）Agent学习，3）记忆，4）Agent通用行为预测，5）认知，如图5所示。

### **4.2.2 Agent Infrastructure**

Agent基于AI在娱乐、研究和工业领域内是一个庞大且快速增长的社区。大型基础模型的发展显著提高了Agent AI系统的性能。然而，这种方式下创建Agent的努力受限于创建高质量数据集所需的不断增加的努力和总体成本。在微软，建造高质量的Agent基础设施通过使用先进的硬件、多样化的数据源及强大的软件库显著影响了多模态Agent copilot。随着微软继续推动Agent技术的边界，AI Agent平台预计将在未来数年内继续在多模态智能世界中保持主导地位。尽管如此，当前的Agent AI交互仍然是一个需要结合多种技能的复杂过程。最近在大型生成性AI模型空间的进展有潜力大幅降低目前互动内容的高成本和所需时间，不仅对大型工作室来说如此，也使得更小的独立内容创造者有能力设计出超出他们当前能力的高质量体验。当前多模态Agent内部的人机交互系统主要是基于规则的。它们确实在一定程度上对人类/用户的动作有智能行为反应，并具有一些网络知识。然而，这些交互往往受限于软件开发成本，以启用系统中的特定行为。此外，当前模型未设计为帮助人类在用户无法完成特定任务时实现目标。因此，需要一个Agent AI系统基础架构来分析用户行为，当需要时提供适当的支持。

## **4.3 Agentic Foundation Models (pretraining and finetune level)**

预训练的基础模型在广泛适用于多样化用例方面提供了显著优势。这些模型的集成使得开发针对各种应用的定制解决方案成为可能，从而规避了为每个特定任务创建大量标记数据集的需求。

在导航领域的一个显著例子是LM-Nav系统（Shah等人，2023a年），该系统结合了GPT-3和CLIP，采用了一种新颖的方法。它有效地使用由语言模型生成的文本地标，并将它们锚定在机器人用于导航的图像中。这种方法展现了文本和视觉数据的无缝融合，显著增强了机器人导航的能力，并保持了广泛的适用性。

在机器人操控方面，有几项研究建议使用现成的LLM（例如，ChatGPT），同时使用开放词汇的对象探测器。LLM与先进对象探测器（例如，Detic（Zhou等，2022年））的结合促进了对人类指令的理解，同时将文本信息与场景信息联系起来（Parakh等，2023年）。此外，最新的进展展示了使用提示工程与先进的多模态模型（例如GPT-4V(ision)（Wake等，2023b年））的潜力。这种技术为多模态任务规划开辟了道路，突出了预训练模型在多种情境中的多功能性和适应性。

# 5 **Agent AI Categorization**

## **5.1 Generalist Agent Areas**

基于计算机的行动Agent和通用Agent（GAs）适用于许多任务。大型基础模型和交互式AI领域的最新进展使GAs获得了新的功能。然而，要使GA真正对用户有价值，它必须易于与人交互，并能泛化到广泛的上下文和模态。我们在第6章对Agent基础AI进行了高质量扩展，尤其是在与这些主题总体相关的领域：

多模态Agent AI（MMA）是即将到来的论坛，我们的研究和行业社区将在这里相互交流，并与更广泛的研究和技术社区进行Agent AI的交流。大型基础模型和交互式AI领域的最新进展使通用Agent（GAs）能够获得新功能，例如在有限制的设置中预测用户行动和任务规划（例如，MindAgent（Gong等，2023a）），细粒度多模态视频理解（Luo等，2022），机器人学（Ahn等，2022b; Brohan等，2023））或提供包含知识反馈的聊天伙伴（例如，为医疗系统的网站客户支持（Peng等，2023））。更多关于代表性工作和最新代表性工作的细节将在下文展示。我们希望讨论我们对MAA未来的愿景，并激励未来的研究者在这个领域工作。本文和我们的论坛涵盖以下主要话题，但不仅限于这些：
• 主要话题：多模态Agent AI，通用Agent AI
• 次要话题：具身Agent，行为Agent，基于语言的Agent，视觉和语言Agent，知识和推理Agent，适用于游戏、机器人学、医疗保健等的Agents。
• 扩展话题：视觉导航，模拟环境，重新排列，Agent化基础模型，虚拟现实/增强现实/混合现实，具身视觉与语言。

接下来，我们将展示一份具体的代表性Agent类别清单如下：

## **5.2 Embodied Agents**

我们的生物大脑居住在身体中，而我们的身体在不断变化的世界中移动。具身人工智能的目标是创建像机器人这样的Agent，这些Agent学会创造性地解决需要与环境互动的挑战性任务。尽管这是一个重大的挑战，但深度学习的重要进展和像ImageNet这样的大型数据集日渐增多，已经使得在一些以前被认为棘手的AI任务上取得了超越人类的表现。计算机视觉、语音识别和自然语言处理已经在像语言翻译和图像分类这样的被动输入输出任务上经历了变革性的革命，强化学习同样在像游戏玩耍这样的互动任务上实现了世界级的表现。这些进步极大地促进了具身AI的发展，使越来越多的用户能够迅速向智能Agent进步，这些Agent能够与机器互动。
****

### **5.2.1 Action Agents**

行动Agent指的是需要在模拟的物理环境或现实世界中执行物理动作的Agent。特别是，它们需要积极地与环境进行互动。我们根据应用领域将行动Agent大致分为两类：游戏AI和机器人学。

在游戏AI中，Agent将与游戏环境以及其他独立实体进行互动。在这些设置中，自然语言可以使Agent和人类之间的沟通更加顺畅。根据游戏的不同，可能会有一个具体的任务需要完成，提供真实的奖励信号。例如，在竞争激烈的外交游戏中，通过使用人类对话数据训练一个语言模型，并结合RL来培训一个行动策略，可以实现与人类相当的游戏水平（Meta Fundamental AI Research (FAIR) 外交团队等，2022）。

还有一些设置是我们的Agent作为小镇中的普通居民行动（Park等，2023a），而不是试图优化一个特定的目标。基础模型在这些设置中非常有用，因为它们可以通过模仿人类行为来模拟看起来更自然的互动。当结合外部记忆时，它们能产生令人信服的Agent，这些Agent可以进行对话、安排日常计划、形成关系，并拥有虚拟生活。

### **5.2.2 Interactive Agents**

互动Agent简单来说就是可以与世界互动的Agent，比行动Agent这个类别更宽泛。它们互动的形式不一定需要物理动作，但可能涉及向用户传达信息或改变环境。例如，一个具身的互动Agent可能会通过对话回答用户关于某个主题的问题，或者像聊天机器人一样帮助用户解析现有信息。通过扩展Agent的能力以包括信息共享，Agent AI的核心设计和算法可以有效适应一系列应用，如诊断（Lee等，2023）和知识检索（Peng等，2023）Agent。

## **5.3 Simulation and Environments Agents**

AI Agent学习如何在环境中行动的一种有效方法是通过与环境的互动经历反复试错的过程。一个代表性的方法是强化学习（RL），它需要大量的失败经验来训练Agent。虽然存在使用物理Agent的方法（Kalashnikov等，2018），使用物理Agent既耗时又成本高昂。此外，在实际环境中可能存在危险的情况下，例如自动驾驶、水下航行器，实际环境中的训练通常是可行的。因此，使用模拟器来学习策略是一种常见的方法。

已经提出了许多用于具身AI研究的仿真平台，范围从导航（Tsoi等，2022；Deitke等，2020；Kolve等，2017）到物体操纵（Wang等，2023d；Mees等，2022；Yang等，2023a；Ehsani等，2021）。一个例子是Habitat (Savva等，2019；Szot等，2021)，它提供了一个3D室内环境，人类和机器人Agent可以执行不同的任务，如导航、遵循指令和回答问题。另一个具有代表性的仿真平台是VirtualHome（Puig等，2018），支持在3D室内环境中进行物体操纵的人类化身。在游戏领域，Carroll等人介绍了"Overcooked-AI"，这是一个旨在研究人类和AI之间合作任务的基准环境（Carroll等，2019）。沿着类似的路线，一些工作致力于结合真实人类的干预，超出了Agent与环境互动的重点（Puig等，2023；Li等，2021a；Srivastava等，2022）。这些模拟器有助于在涉及Agent和机器人互动的实际设置中学习策略，以及利用人类示范行为的IL-based策略学习。

在某些情况下，学习策略的过程可能需要在模拟器中集成专门的功能。例如，学习基于图像的政策，在这种情况下，通常需要现实渲染来促进适应真实环境（Mittal等，2023；Zhong等，2023）。使用真实渲染引擎对于生成反映各种条件（如光照环境）的图像非常有效。此外，需要使用物理引擎的模拟器来模拟与物体的物理互动（Liu和Negrut，2021）。在模拟中整合物理引擎已经被证明有助于获取在现实世界情境中适用的技能（Saito等，2023）。

## **5.4 Generative Agents**

大型生成AI模型的最新进展有潜力大幅度降低当前交互式内容的高成本和时间需求，这不仅适用于大型游戏工作室，也能使小型独立工作室创造出超出其当前能力范围的高质量体验。此外，将大型AI模型嵌入沙盒环境将允许用户创作自己的体验，并以当前无法达到的方式表达他们的创造力。

这个Agent的目标不仅仅是在场景中添加交互式3D内容，还包括：

- 为对象添加任意的行为和互动规则，允许用户仅通过最少的提示创建自己的VR规则。
- 利用多模态的GPT4-v模型，从一张纸上的草图生成整个关卡的几何结构，以及其他涉及视觉AI模型链的模型。
- 使用扩散模型对场景中的内容进行重新贴图。
- 从简单的用户提示创建自定义着色器和视觉特效。

短期内的一个潜在应用是VR故事板/原型工具的创建，允许单个用户创建一个粗糙（但功能性的）体验/游戏草图，其速度比当前可行的速度快一个数量级。这样的原型可以使用这些工具进行扩展并进行更精细的打磨。

### **5.4.1 AR/VR/mixed-reality Agents**

当前的AR/VR/混合现实（统称为XR）环境需要有技艺高超的艺术家和动画师来创造角色、环境和物体，以便在虚拟世界中模拟交互。这一过程成本高昂，包括概念艺术、3D建模、贴图、绑定和动画制作。XR Agent可以在这一过程中提供帮助，通过促进创造者之间的交互并构建工具来帮助建立最终的虚拟环境。

我们的早期实验已经显示，GPT模型甚至可以在Unity引擎中采用几次学习（few-shot）的方式被使用（无需任何额外的微调），来调用特定于该引擎的方法，使用API调用从互联网下载3D模型并将其置入场景中，并为它们指定行为和动画的状态树（Huang et al., 2023a）。这种行为很可能是由于开源游戏库中存在使用Unity的类似代码而产生的。因此，GPT模型能够在视觉场景构建中表现出其丰富性，从简单的用户提示中加载许多对象到场景中。

此类Agent的目标是构建一个平台和一套工具，提供一个高效的界面，将大型AI模型（包括GPT系列和扩散图像模型）与渲染引擎相连接。我们在这里探索两个主要的途径：

- 将大型模型集成到Agent基础设施中的各种编辑工具中，允许显着加速开发。
- 通过在用户体验中控制渲染引擎，通过生成遵循用户指令的代码，然后在运行时编译它，从而允许用户以任意方式编辑他们正在交互的VR/模拟环境，甚至是引入新的Agent机制。

引入一个专注于XR设置的AI副驾驶将对XR创造者非常有用，他们可以使用副驾驶来完成繁琐的任务，如提供简单的资产或编写代码模板，释放创造者专注于他们的创意视野，并迅速迭代想法。

此外，Agent可以帮助用户交互性地修改环境，通过添加新的资产、改变环境的动态或建立新的设置。这种在运行时的动态生成也可以由创造者指定，使得用户的体验感觉新鲜并不断进化。

## **5.5 Knowledge and Logical Inference Agents**

人类认知的一个显著特点是推断和应用知识的能力，特别是在逻辑推理和理解心理理论等复杂任务中尤为明显。在知识上进行推理确保了AI的响应和行动与已知事实和逻辑原则一致。这种连贯性是维持AI系统信任和可靠性的关键机制，特别是在医疗诊断或法律分析等关键应用中尤为重要。在这里，我们介绍的Agent融合了知识与推理的相互作用，解决了智能和推理的特定方面。

### **5.5.1 Knowledge Agent**

知识Agent在其获取的知识体系上进行推理，分为隐性和显性两个方向。隐性知识通常是像GPT系列这样的大规模语言模型（Brown et al., 2020; OpenAI, 2023）在训练过程中积累的，它们通过海量的文本数据训练而得。这些模型能够生成给人以理解印象的回应，因为它们利用了在训练过程中隐性学习到的模式和信息。相反，显性知识则是结构化的，可以直接查询，例如在知识库或数据库中找到的信息，这种信息传统上被用来增强AI的推理能力，通过引用可验证的外部资源。

尽管语言模型有了进步，它们的隐性知识仍然是静态的，并且随着世界的发展而过时（Lewis et al., 2020; Peng et al., 2023）。这个局限性需要集成不断更新的显性知识来源，以确保AI系统能够提供准确和最新的响应。隐性和显性知识的融合使AI Agent具备了更细腻的理解力和对知识进行情境应用的能力，类似于人类的智能（Gao et al., 2022）。这种集成对于打造以知识为中心的AI Agent至关重要，这些Agent不仅拥有信息，还能理解、解释和使用这些信息，从而缩小了广泛学习与深刻知识之间的鸿沟（Marcus and Davis, 2019; Gao et al., 2020）。这些Agent被设计为能够灵活地推理，并处理有关世界的动态信息，提高了它们的强大性和适应性（Marcus, 2020）。

### **5.5.2 Logic Agents**

通常而言，逻辑Agent是一个系统的组件，旨在应用逻辑推理来处理数据或解决特定于逻辑推理或逻辑推断的任务。在像GPT-4这样的大型基础模型的背景下，逻辑Agent指的是专门设计来处理逻辑推理任务的组件或子模块。这些任务经常涉及理解和操纵抽象概念，从给定的前提中演绎结论，或解决需要结构化、逻辑方法的问题。广义上，像GPT-4这样的基础模型经过大量文本数据的训练，学会执行包括逻辑推理在内的广泛任务。因此，它们的逻辑推理能力被集成到整个架构中，并且通常不具备一个独立、隔离的“逻辑Agent”。

尽管GPT-4和类似模型可以执行涉及逻辑的任务，但它们的方法从根本上与人类或传统基于逻辑的系统的操作方式不同。它们不遵循正式的逻辑规则或对逻辑有一个明确的理解；相反，它们是基于从训练数据中学到的模式来生成响应。因此，它们在逻辑任务中的表现可能令人印象深刻，但也可能因训练数据的性质和模型设计的固有局限性而变得不一致或受限。

在架构中嵌入一个独立的逻辑子模块的一个例子是（Wang et al., 2023e），该研究修改了LLM在预训练期间使用的token嵌入过程，通过将文本解析成逻辑段落，并在token嵌入中显式地建模逻辑层次结构。

### **5.5.3 Agents for Emotional Reasoning**

在多种人机交互中，情感理解和同理心是Agent的重要技能。例如，创建能够引发用户参与的对话Agent的一个重要目标是让Agent表现出更多的情感和同理心，同时尽量减少社交不当或冒犯性的输出。为了向这个目标迈进，我们发布了一个名为Neural Image Commenting with Empathy (NICE) 的数据集（Chen等人，2021年），该数据集包含了近两百万幅图像以及相应的人类生成评论和一组人类情感标注。我们还提供了一种新颖的预训练模型——Modeling Affect Generation for Image Comments (MAGIC)（Chen等人，2021年）——该模型旨在基于捕捉风格和情感的语言表征来为图像生成评论，帮助生成更具同理心、情感丰富、引人入胜以及社会适宜的评论。我们的实验表明，这种方法在训练更像人类的、更具吸引力的图像评论Agent方面是有效的。开发具有同理心意识的Agent是交互性Agent的一个有前途的方向，创建在情感理解能力上横跨广泛群体和人群的Agent尤为重要，尤其是考虑到许多当前的语言模型在情感理解和具有同理心的推理能力上表现出偏见（Mao等人，2022年；Wake等人，2023d年）。

### **5.5.4 Neuro-Symbolic Agents**

神经符号（Neuro-Symbolic）Agent基于神经元和符号的混合系统运作（d’Avila Garcez和Lamb，2020年）。解决自然语言中提出的问题是一项具有挑战性的任务，因为它需要显式地捕获输入中隐含的离散符号结构信息。然而，大多数通用神经序列模型并不显式地捕获此类结构信息，这限制了它们在这些任务上的表现。Chen等人在2020年的工作提出了一种新的基于结构化神经表征的编码器-解码器模型。编码器TP-N2F采用张量积表示（TPR）‘绑定’来在向量空间内编码自然语言的符号结构，而解码器使用TPR‘解绑’来在符号空间内生成，由关系（或操作）和多个参数组成的关系元组序列化程序表示。

指令遵循视觉-语言（VL）模型，如GPT-4，提供了一个灵活的界面，支持多种模态任务的零次（zero-shot）方式。然而，那些在整个图像上操作的界面并不能直接让用户“指向”并访问图像内的特定区域。这种能力不仅对于支持参考基准的视觉-语言模型很重要，而且对于需要精确的图像内推理的实践应用也很重要。在Park等人2023年的工作中，我们构建了一个可以让用户指定（多个）区域作为输入的Localized Visual Commonsense模型。我们通过从大型语言模型（LLM）中采样局部常识知识来训练我们的模型：具体来说，我们提示LLM在给定全局字面意义的图像描述和由一系列VL模型自动生成的局部字面意义区域描述的情况下收集常识知识。这一流程是可扩展的且完全自动的，因为它不需要对齐或人工编写的图像和文本对。通过一个单独训练的批评模型选取高质量示例，我们发现仅从图像扩展的局部常识数据库上进行训练可以成功提炼现有的VL模型，以支持以参考为输入的界面。零次设置中的经验结果和人类评估表明，我们的提炼方法与生成指示语句的基线相比，结果是更精确的VL模型推理。

## **5.6 LLMs and VLMs Agent**

已有多项工作利用大型语言模型（LLM）作为Agent来执行任务规划（Huang等人，2022a；Wang等人，2023b；Yao等人，2023a；Li等人，2023a），并利用LLM庞大的互联网规模领域知识和零次规划能力来执行代理任务，如规划和推理。近期的机器人研究也利用LLM进行任务规划（Ahn等人，2022a；Huang等人，2022b；Liang等人，2022），方法是将自然语言指令分解为一系列子任务，这些子任务要么以自然语言的形式，要么以Python代码的形式表达，然后使用低级控制器执行这些子任务。此外，Huang等人（2022b）、Liang等人（2022）以及Wang等人（2023a）的工作还结合了环境反馈来提高任务性能。还有一些工作展示了以大规模文本、图像和视频数据训练的通用视觉对齐大型语言模型的能力，这些模型可以作为创建多模态Agent的基础，这些Agent是具身的，并且能够在各种环境中行动（Baker等人，2022；Driess等人，2023；Brohan等人，2023）。